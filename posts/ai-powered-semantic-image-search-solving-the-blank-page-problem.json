{
  "title": "AI-Powered Semantic Image Search: Solving the Blank Page Problem",
  "slug": "ai-powered-semantic-image-search-solving-the-blank-page-problem",
  "content": "<p>Have you ever stared at a stock photo search bar, knowing exactly the <em>vibe</em> you want but struggling to find the right keywords? You type \"sad girl,\" and get generic, staged results. You try \"rainy window,\" and get landscapes. The disconnect between your mental image and the search engine's literal keyword matching is the \"Blank Page Problem.\"</p><p>I built <strong>Semantic Image Search</strong> to fix this. It's a local-first, privacy-focused search engine that uses the power of Large Language Models (LLMs) to understand what you <em>mean</em>, not just what you type.</p><p></p><h2>The Problem with Keywords</h2><p>Traditional stock photo search engines rely on metadata tags. If a photographer tagged an image with \"blue,\" \"sky,\" and \"summer,\" searching for \"azure horizon\" might miss it entirely. As developers and creators, we waste hours tweaking queries: \"happy office,\" \"diverse team collaboration,\" \"modern workspace setup\"... it's an endless guessing game.</p><p></p><h2>The Solution: Semantic Expansion</h2><p>My project keeps the search logic local and intelligent. Instead of sending your raw query directly to a provider, it passes through a local AI layer powered by <strong>Ollama</strong> running <strong>Gemma 3 (4b)</strong>.</p><p>Here looks the difference:</p><p><strong>User Input:</strong> \"cyberpunk city\"</p><p><strong>AI Enhanced Query:</strong> <em>\"futuristic neon-lit cityscape at night with rain, cyberpunk aesthetic, blade runner style, high contrast, cinematic lighting, 8k resolution, highly detailed\"</em></p><p>This enhanced prompt is what actually gets sent to the APIs, resulting in images that match your <em>intent</em>, not just your words.</p><h2>Under the Hood: The Micro-Orchestrator</h2><p>The system is built on a <strong>Micro-Orchestrator Pattern</strong>. This means a central intelligence unit manages the flow of data without blocking the user experience.</p><img class=\"mx-auto rounded-lg max-w-full\" src=\"https://raw.githubusercontent.com/tamilarasu18/tamilarasu-portfolio-blog/main/media/2026-02-05/1770303560553-blog-center-image-1770303024302.png\"><p></p><h2>Architecture Highlights</h2><p>1.  <strong>FastAPI Gateway</strong>: A high-performance async API that receives your request.</p><p>2.  <strong>Local AI Layer</strong>: An intent engine using Ollama translates vague ideas into professional photography terms.</p><p>3.  <strong>Parallel Execution</strong>: The system queries <strong>Unsplash</strong>, <strong>Pexels</strong>, and <strong>Pixabay</strong> simultaneously.</p><p>4.  <strong>Resilience</strong>: Built with exponential backoff and circuit breaking. If one provider is down (API limits, network error), the system gracefully degrades and returns results from the others.</p><p></p><h3>Why Local?</h3><p>You might ask, \"Why run this locally instead of using OpenAI's API?\"</p><p>- <strong>Privacy</strong>: Your creative prompts and search history never leave your machine.</p><p>- <strong>Cost</strong>: It costs $0. Running a 4b parameter model like Gemma 3 is feasible on most modern laptops.</p><p>- <strong>Control</strong>: You can swap the model for Llama 3, Mistral, or any other supported model in seconds.</p><p></p><h2>Tech Stack</h2><p>- <strong>Framework</strong>: FastAPI</p><p>- <strong>LLM Runtime</strong>: Ollama</p><p>- <strong>Language</strong>: Python 3.11+</p><p>- <strong>Resilience</strong>: Tenacity library for retries</p><p>- <strong>Validation</strong>: Pydantic</p><p></p><h2>Getting Started</h2><p>The project is open source and easy to set up.</p><p>1.  <strong>Clone the repo</strong>: <code>git clone https://github.com/tamilarasu18/ollama-semantic-image-search</code></p><p>2.  <strong>Install dependencies</strong>: <code>pip install -r requirements.txt</code></p><p>3.  <strong>Run Ollama</strong>: <code>ollama pull gemma3:4b</code></p><p>4.  <strong>Start Search</strong>: <code>uvicorn main:app --reload</code></p><p>Check out the code on [GitHub](<a target=\"_blank\" rel=\"noopener noreferrer nofollow\" class=\"text-[#2563EB] hover:underline cursor-pointer\" href=\"https://github.com/tamilarasu18/ollama-semantic-image-search\">https://github.com/tamilarasu18/ollama-semantic-image-search</a>) and stop fighting with keywords. Let AI do the searching for you.</p><p></p>",
  "excerpt": "Have you ever stared at a stock photo search bar, knowing exactly the vibe you want but struggling to find the right keywords? You type \"sad girl,\" and get gene...",
  "coverImage": "https://raw.githubusercontent.com/tamilarasu18/tamilarasu-portfolio-blog/main/media/2026-02-05/1770303442803-blog-cover-image-1770303132872.png",
  "author": {
    "name": "Tamilarasu",
    "avatar": "/avatar.png"
  },
  "createdAt": "2026-02-05T15:01:35.370Z",
  "updatedAt": "2026-02-05T15:01:35.370Z",
  "tags": [
    "ai",
    "ollama",
    "semantic image search",
    "github",
    "local model"
  ],
  "readingTime": 3,
  "engagement": {
    "likes": 1,
    "dislikes": 0
  },
  "comments": [
    {
      "id": "comment-1770304196827-omb0bzy8p",
      "author": "Mahadeer",
      "content": "Your explanation clearly highlights a real and relatable problem with traditional stock photo search.\nThe transition to your solution is concise and easy to understand.\nMentioning Ollama and Gemma 3 (4b) adds credibility and technical depth.\nOverall, the concept feels modern, practical, and immediately valuable for creators and developers.\n\nThank you so much",
      "createdAt": "2026-02-05T15:09:56.827Z"
    }
  ]
}